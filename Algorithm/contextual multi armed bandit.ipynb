{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "788fcdda",
   "metadata": {},
   "source": [
    "# Contextual Multi-armed bandit algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e066ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import math\n",
    "\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4779f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ind_max(temp):\n",
    "    m = max(temp)\n",
    "    list_m = list(filter(lambda x: temp[x] == m, range(len(temp))))\n",
    "    return random.choice(list_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce611aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reward(arm, x, true_theta):\n",
    "    p = true_theta.dot(x) + np.random.normal(scale=0.01)\n",
    "    return p[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2f78799",
   "metadata": {},
   "source": [
    "# Algorithms\n",
    "## Lin UCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2da4f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_UCB(alpha,n_simulation, n_trials,n_arms, n_features):\n",
    "    n_simulation = n_simulation\n",
    "    n_trials = n_trials\n",
    "    n_arms = n_arms\n",
    "    n_features = n_features\n",
    "    \n",
    "    # making empty storage for writing result of simulation\n",
    "    sim_nums = [0.0 for i in range(n_simulation * n_trials)]\n",
    "    times = [0.0 for i in range(n_simulation * n_trials)]\n",
    "    chosen_arms = [0.0 for i in range(n_simulation * n_trials)]\n",
    "    rewards = [0.0 for i in range(n_simulation * n_trials)]\n",
    "    cumulative_rewards = [0.0 for i in range(n_simulation * n_trials)]\n",
    "    \n",
    "    true_theta = np.array([[ 0.34, -0.12, -0.16,  0.32,  0.03,  0.19, -0.31, -0.70, 0.62,  0.19]])\n",
    "    \n",
    "    for sim in range(n_simulation):\n",
    "        sim = sim + 1\n",
    "        \n",
    "        X = np.array([[np.random.uniform(size = n_features) for _ in np.arange(n_arms)] for _ in np.arange(n_trials)])\n",
    "        A = np.identity(n_features) \n",
    "        b = np.zeros(shape=(n_features,1))    \n",
    "        \n",
    "        for t in range(n_trials):\n",
    "            index = (sim - 1) * n_trials + t\n",
    "            sim_nums[index] = sim\n",
    "            times[index] = t+1\n",
    "            \n",
    "            # select arm\n",
    "            p = [0 for i in range(n_arms)]\n",
    "            inv_A = np.linalg.inv(A)\n",
    "            theta = inv_A.dot(b.reshape(n_features,))\n",
    "            \n",
    "            if t%100 == 99:\n",
    "                print('simulation number : ', sim)\n",
    "                print(t,' : theta : ', np.round(theta,3))\n",
    "                \n",
    "            for a in range(n_arms):\n",
    "                mean = theta.dot(X[t, a])\n",
    "                p[a] = mean + alpha * np.sqrt(X[t, a].dot(inv_A).dot(X[t, a]))\n",
    "            chosen_arm = ind_max(p)\n",
    "            context_t = X[t, chosen_arm]\n",
    "\n",
    "            # get the reward\n",
    "            reward = generate_reward(arm=0, x = context_t, true_theta=true_theta) \n",
    "            rewards[index] = reward\n",
    "            \n",
    "            if t == 0:\n",
    "                cumulative_rewards[index] = reward\n",
    "            else:\n",
    "                cumulative_rewards[index] = cumulative_rewards[index - 1] + reward\n",
    "            chosen_arms[index] = chosen_arm\n",
    "            \n",
    "            context_t = np.reshape(context_t, (-1, 1))\n",
    "            A += context_t.dot(context_t.T)\n",
    "            b += reward*context_t\n",
    "\n",
    "    temp_result =  [sim_nums, times, chosen_arms, rewards, cumulative_rewards]\n",
    "    result = pd.DataFrame(temp_result).transpose()\n",
    "    result.columns = [\"Sim\", \"T\", \"ChosenArm\", \"Reward\", \"CumulativeReward\"]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03076fbf",
   "metadata": {},
   "source": [
    "## Thmopson Sampling with linear pay off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d414913d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lin_TS(delta, R,EPSILON, n_simulation,n_trials, n_arms,n_features ):\n",
    "    n_simulation = n_simulation\n",
    "    n_trials = n_trials\n",
    "    n_arms = n_arms\n",
    "    n_features = n_features\n",
    "    v = R * np.sqrt(24/EPSILON*d*np.log(1/ delta)) \n",
    "    \n",
    "    # making empty storage for writing result of simulation\n",
    "    sim_nums = [0.0 for i in range(n_simulation * n_trials)]\n",
    "    times = [0.0 for i in range(n_simulation * n_trials)]\n",
    "    chosen_arms = [0.0 for i in range(n_simulation * n_trials)]\n",
    "    rewards = [0.0 for i in range(n_simulation * n_trials)]\n",
    "    cumulative_rewards = [0.0 for i in range(n_simulation * n_trials)]\n",
    "    \n",
    "    true_theta = np.array([[ 0.34, -0.12, -0.16,  0.32,  0.03,  0.19, -0.31, -0.70, 0.62,  0.19]])\n",
    "    \n",
    "    for sim in range(n_simulation):\n",
    "        sim = sim + 1\n",
    "        \n",
    "        X = np.array([[np.random.uniform(size = n_features) for _ in np.arange(n_arms)] for _ in np.arange(n_trials)])\n",
    "        A = np.identity(n_features) \n",
    "        b = np.zeros(shape=(n_features,1))    \n",
    "        \n",
    "        for t in range(n_trials):\n",
    "            index = (sim - 1) * n_trials + t\n",
    "            sim_nums[index] = sim\n",
    "            times[index] = t+1\n",
    "            \n",
    "            # select arm\n",
    "            inv_A = np.linalg.inv(A)\n",
    "            mean = np.dot(inv_A, b).flatten()\n",
    "            cov = v**2 * inv_A\n",
    "            theta = np.random.multivariate_normal(mean, cov).reshape((n_features,))\n",
    "            \n",
    "            if t%100 == 99:\n",
    "                print('simulation number : ', sim)\n",
    "                print(t,': theta : ',np.round(theta,3))\n",
    "                \n",
    "            p = [0 for i in range(n_arms)]\n",
    "            for a in range(n_arms):\n",
    "                p[a] = theta.dot(X[t, a])      \n",
    "            chosen_arm = ind_max(p)\n",
    "            context_t = X[t, chosen_arm]\n",
    "            \n",
    "            # get the reward\n",
    "            reward = generate_reward(arm=0, x = context_t, true_theta=true_theta) \n",
    "            rewards[index] = reward\n",
    "            \n",
    "            if t == 0:\n",
    "                cumulative_rewards[index] = reward\n",
    "            else:\n",
    "                cumulative_rewards[index] = cumulative_rewards[index - 1] + reward\n",
    "            chosen_arms[index] = chosen_arm\n",
    "            \n",
    "            context_t = np.reshape(context_t, (-1, 1))\n",
    "            A += context_t.dot(context_t.T)\n",
    "            b += reward*context_t\n",
    "\n",
    "    temp_result =  [sim_nums, times, chosen_arms, rewards, cumulative_rewards]\n",
    "    result = pd.DataFrame(temp_result).transpose()\n",
    "    result.columns = [\"Sim\", \"T\", \"ChosenArm\", \"Reward\", \"CumulativeReward\"]\n",
    "    return result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
